{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MaximumEntropyMarkovModel.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEkI_j5UIE00",
        "outputId": "eb16c7f1-3fd8-4178-e7c8-1cb4076cbe38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def conll_sentences(conll_file):\n",
        "    sent = []\n",
        "    pos = []\n",
        "    chunk = []\n",
        "    entity = []\n",
        "    temp_sent = []\n",
        "    temp_pos = []\n",
        "    temp_chunk = []\n",
        "    temp_entity = []\n",
        "    \n",
        "    with open(conll_file) as f:\n",
        "        conll_raw_data = f.readlines()\n",
        "    conll_raw_data = [x.strip() for x in conll_raw_data]\n",
        "\n",
        "    for line in conll_raw_data:\n",
        "        if line != '':\n",
        "            split_line = line.split()\n",
        "            if len(split_line) == 4:\n",
        "                if split_line[0] != '-DOCSTART-':\n",
        "                    temp_sent.append(split_line[0])\n",
        "                    temp_pos.append(split_line[1])\n",
        "                    temp_chunk.append(split_line[2])\n",
        "                    \n",
        "                    # Rename entity values as PER, LOC, ORG, MISC, O\n",
        "                    old_ent = split_line[3]\n",
        "                    if old_ent in ('I-ORG', 'B-ORG'):\n",
        "                        new_ent = 'ORG'\n",
        "                    elif old_ent in ('I-LOC', 'B-LOC'):\n",
        "                        new_ent = 'LOC'\n",
        "                    elif old_ent in ('I-MISC', 'B-MISC'):\n",
        "                        new_ent = 'MISC'\n",
        "                    elif old_ent in ('I-PER', 'B-PER'):\n",
        "                        new_ent = 'PER'\n",
        "                    else:\n",
        "                        new_ent = 'O'\n",
        "                    temp_entity.append(new_ent)\n",
        "            else:\n",
        "                raise IndexError('Line split length does not equal 4.')\n",
        "        else:\n",
        "            if len(temp_sent) > 0:\n",
        "                assert(len(sent) == len(pos))\n",
        "                assert(len(sent) == len(chunk))\n",
        "                assert(len(sent) == len(entity))\n",
        "                sent.append(temp_sent)\n",
        "                pos.append(temp_pos)\n",
        "                chunk.append(temp_chunk)\n",
        "                entity.append(temp_entity)\n",
        "                temp_sent = []\n",
        "                temp_pos = []\n",
        "                temp_chunk = []\n",
        "                temp_entity = []\n",
        "    \n",
        "    return sent, pos, chunk, entity"
      ],
      "metadata": {
        "id": "02ie5x-PJw2C"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Z83dU2G4J6vo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get CoNLL Features"
      ],
      "metadata": {
        "id": "rJTQiGDnSH0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_conll_features(index, sentence, pos, chunk):\n",
        "  \"\"\"Function used to extract features for the CoNLL dataset\n",
        "    \n",
        "    'w' represents word feature\n",
        "    't' represents POS tag feature\n",
        "    'c' represents chunk tag feature\n",
        "    '-n' represents previous 'n' feature\n",
        "    '+n' represents posterior 'n' feature\n",
        "  \"\"\"\n",
        "\n",
        "  features = {}\n",
        "  last_index = len(sentence) - 1\n",
        "  word = sentence[index]\n",
        "  word_lc = word.lower()\n",
        "\n",
        "  # features from current word:\n",
        "\n",
        "  features[\"w\"] = word\n",
        "  features[\"t\"] = pos[index]\n",
        "  features[\"length\"] = len(word)\n",
        "  features[\"upperase\"] = any(x.isupper() for x in word)\n",
        "  features[\"firstletter\"] = word[0].isupper() and (len(word)>1)\n",
        "  features[\"hasdigits\"] = any(x.isdigit() for x in word)\n",
        "  features[\"c\"] = chunk[index]\n",
        "  features[\"log_flag\"] = (\"field\" in word_lc) or (\"land\" in word_lc) or (\"burgh\" in word_lc) or (\"shire\" in word_lc)\n",
        "  features[\"hasdot\"] = (\".\" in word and len(word) > 1)\n",
        "  features[\"endsinns\"] = (len(word)>1 and word_lc[-2:]=='ns')\n",
        "\n",
        "\n",
        "  # features from previous 2 words\n",
        "  if index == 0: #first word in sentence\n",
        "    features[\"t-2 t-1\"] = \"<B> <B>\"\n",
        "    features[\"t-1\"] = \"<B>\"\n",
        "    features[\"w-2\"] = \"<B>\"\n",
        "    features[\"w-1\"] = \"<B>\"\n",
        "    features[\"c-2 c-1\"] = \"<B> <B>\"\n",
        "    features[\"c-1\"] = \"<B>\"\n",
        "\n",
        "  elif index == 1: #second word in sentence\n",
        "    features[\"t-2 t-1\"] = \"<B> \"+pos[0]\n",
        "    features[\"t-1\"] = pos[0]\n",
        "    features[\"w-2\"] = \"<B>\"\n",
        "    features[\"w-1\"] = sentence[0]\n",
        "    features[\"c-2 c-1\"] = \"<B> \"+chunk[0]\n",
        "    features[\"c-1\"] = chunk[0]\n",
        "  else:\n",
        "    features[\"t-2 t-1\"] = pos[index-2] + ' ' + pos[index-1]\n",
        "    features[\"t-1\"] = pos[index-1]\n",
        "    features[\"w-2\"] = sentence[index-2]\n",
        "    features[\"w-1\"] = sentence[index-1]\n",
        "    features[\"c-2 c-1\"] = chunk[index-2] + ' ' + chunk[index-1]\n",
        "    features[\"c-1\"] = chunk[index-1]\n",
        "\n",
        "  # features from posterior 2 words\n",
        "  if index == last_index: #last word in sentence\n",
        "    features[\"t+1 t+2\"] = \"<E> <E>\"\n",
        "    features[\"t+1\"] = \"<E>\"\n",
        "    features[\"w+2\"] = \"<E>\"\n",
        "    features[\"w+1\"] = \"<E>\"\n",
        "\n",
        "  elif index == last_index - 1: #second last word in sentence\n",
        "    features[\"t+1 t+2\"] = pos[last_index] + \"<E>\"\n",
        "    features[\"t+1\"] = pos[last_index]\n",
        "    features[\"w+2\"] = \"<E>\"\n",
        "    features[\"w+1\"] = sentence[last_index]\n",
        "  else:\n",
        "    features[\"t+1 t+2\"] = pos[index+1] + ' ' + pos[index+2]\n",
        "    features[\"t+1\"] = pos[index+1]\n",
        "    features[\"w+2\"] = sentence[index+2]\n",
        "    features[\"w+1\"] = sentence[index+1]\n",
        "\n",
        "  return features\n"
      ],
      "metadata": {
        "id": "2xFHgKNaJ8vf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = r\"/content/drive/MyDrive/Dataset/NER_Dataset/CoNLL2003/eng.train\"\n",
        "\n",
        "train_sent, train_pos, train_chunk, train_entity = conll_sentences(train_file)\n"
      ],
      "metadata": {
        "id": "fz52lAMEOt0V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Gx9eN3_EQF01"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = []\n",
        "for sent, pos, chunk, entity in zip(train_sent, train_pos, train_chunk, \n",
        "                                    train_entity):\n",
        "  if len(sent) != len(pos) or len(pos) != len(chunk) or len(chunk) != len(entity):\n",
        "    raise ValueError(\"ERROR: CoNLL train length miss match\")\n",
        "  \n",
        "  for i, ent in enumerate(entity):\n",
        "    labelled_data = (get_conll_features(i, sent, pos, chunk), ent)\n",
        "    train_data.append(labelled_data)\n"
      ],
      "metadata": {
        "id": "k_GHBVjQPNtq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Conditional Markov Model NLTK classifier on CoNLL train dataset"
      ],
      "metadata": {
        "id": "vvwngw_0R6bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://hal3.name/megam/megam_i686.opt.gz\n",
        "\n",
        "!gunzip /content/megam_i686.opt.gz \n",
        "\n",
        "!chmod ugo+rx /content/megam_i686.opt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNLFME8GYX0q",
        "outputId": "abb65ce9-4b13-47d8-c7f2-5af36a48df14"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-29 18:24:30--  http://hal3.name/megam/megam_i686.opt.gz\n",
            "Resolving hal3.name (hal3.name)... 64.98.135.72\n",
            "Connecting to hal3.name (hal3.name)|64.98.135.72|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: http://users.umiacs.umd.edu/~hal/megam/megam_i686.opt.gz [following]\n",
            "--2022-07-29 18:24:30--  http://users.umiacs.umd.edu/~hal/megam/megam_i686.opt.gz\n",
            "Resolving users.umiacs.umd.edu (users.umiacs.umd.edu)... 128.8.120.33\n",
            "Connecting to users.umiacs.umd.edu (users.umiacs.umd.edu)|128.8.120.33|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 333329 (326K) [application/x-gzip]\n",
            "Saving to: ‘megam_i686.opt.gz’\n",
            "\n",
            "megam_i686.opt.gz   100%[===================>] 325.52K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2022-07-29 18:24:30 (3.36 MB/s) - ‘megam_i686.opt.gz’ saved [333329/333329]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ajIJojtLeVgY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import os\n",
        "from nltk import MaxentClassifier\n",
        "\n",
        "megam_path = os.path.expanduser(\"/content/megam_i686.opt\")\n",
        "nltk.config_megam(megam_path)\n",
        "maxentClf = MaxentClassifier.train(train_data, algorithm='MEGAM')"
      ],
      "metadata": {
        "id": "3VToGjxvRBAR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vlnV0zOyhg9J"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testa_file = r\"/content/drive/MyDrive/Dataset/NER_Dataset/CoNLL2003/eng.testa\"\n",
        "testb_file = r\"/content/drive/MyDrive/Dataset/NER_Dataset/CoNLL2003/eng.testb\"\n",
        "testc_file = r\"/content/drive/MyDrive/Dataset/NER_Dataset/CoNLL2003/eng.testc\"\n",
        "\n",
        "testa_sent, testa_pos, testa_chunk, testa_entity = conll_sentences(testa_file)\n",
        "testb_sent, testb_pos, testb_chunk, testb_entity = conll_sentences(testb_file)\n",
        "testc_sent, testc_pos, testc_chunk, testc_entity = conll_sentences(testc_file)\n",
        "\n",
        "\n",
        "\n",
        "test_sent = testa_sent + testb_sent + testc_sent\n",
        "test_pos = testa_pos + testb_pos + testc_pos\n",
        "test_chunk = testa_chunk + testb_chunk + testc_chunk\n",
        "test_entity = testa_entity + testb_entity + testc_entity\n"
      ],
      "metadata": {
        "id": "ormayH3ShhEc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "coxe97tYRs27"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_entity_true_pred(test_sent, test_pos, test_chunk, test_entity):\n",
        "  test_truth = []\n",
        "  test_pred = []\n",
        "\n",
        "  for sent, pos, chunk, entity in zip(test_sent, test_pos, test_chunk, test_entity):\n",
        "    if len(sent) != len(pos) or len(pos) != len(chunk) or len(chunk) != len(entity):\n",
        "      raise ValueError(\"ERROR: CoNLL test lentgh miss match\")\n",
        "\n",
        "    for i, ent in enumerate(entity):\n",
        "      test_truth.append(ent)\n",
        "      pred = maxentClf.classify(get_conll_features(i, sent, pos, chunk))\n",
        "      test_pred.append(pred)\n",
        "\n",
        "  return test_truth, test_pred"
      ],
      "metadata": {
        "id": "W9IHR3De2y_R"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_truth, test_pred = get_entity_true_pred(test_sent, test_pos, test_chunk, test_entity)\n"
      ],
      "metadata": {
        "id": "hE54TzB8h-eM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy (expected, predicted):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i in range(len(expected)):\n",
        "        total += 1\n",
        "        if (expected[i] == predicted[i]):\n",
        "            correct += 1\n",
        "    print('accuracy = %d / %d = %lf' % (correct, total, correct/total))"
      ],
      "metadata": {
        "id": "iCEjoALviMfN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy(test_truth, test_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1djrkdhKh5RY",
        "outputId": "a77da103-b45c-465f-f459-a2b8c13aebe6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy = 91110 / 97832 = 0.931290\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jyPxjDo_4dMp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(test_truth, test_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6OHsAC53Y7m",
        "outputId": "ce890222-a7a7-4d36-ff37-8ff21ac712dc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.66      0.67      0.66      4019\n",
            "        MISC       0.70      0.51      0.59      2188\n",
            "           O       0.98      0.98      0.98     81111\n",
            "         ORG       0.64      0.57      0.60      4590\n",
            "         PER       0.74      0.81      0.77      5924\n",
            "\n",
            "    accuracy                           0.93     97832\n",
            "   macro avg       0.74      0.71      0.72     97832\n",
            "weighted avg       0.93      0.93      0.93     97832\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "R12fwShk3Tct"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testa_truth, testa_pred = get_entity_true_pred(testa_sent, testa_pos, testa_chunk, testa_entity)\n",
        "\n",
        "print(accuracy(testa_truth, testa_pred))\n",
        "\n",
        "print(classification_report(testa_truth, testa_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPLFDwcY3TlY",
        "outputId": "0858cd12-b102-4deb-9ee2-5eb740042a83"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy = 48065 / 51362 = 0.935809\n",
            "None\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.66      0.67      0.67      2094\n",
            "        MISC       0.73      0.49      0.59      1268\n",
            "           O       0.98      0.99      0.98     42759\n",
            "         ORG       0.63      0.56      0.59      2092\n",
            "         PER       0.76      0.82      0.79      3149\n",
            "\n",
            "    accuracy                           0.94     51362\n",
            "   macro avg       0.75      0.71      0.72     51362\n",
            "weighted avg       0.93      0.94      0.93     51362\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testb_truth, testb_pred = get_entity_true_pred(testb_sent, testb_pos, testb_chunk, testb_entity)\n",
        "\n",
        "print(accuracy(testb_truth, testb_pred))\n",
        "\n",
        "print(classification_report(testb_truth, testb_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOWWP0w73f0J",
        "outputId": "6bec49b6-09a6-4049-92a2-9622d2196656"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy = 43013 / 46435 = 0.926306\n",
            "None\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.65      0.67      0.66      1925\n",
            "        MISC       0.67      0.54      0.60       918\n",
            "           O       0.98      0.98      0.98     38323\n",
            "         ORG       0.64      0.58      0.61      2496\n",
            "         PER       0.71      0.80      0.75      2773\n",
            "\n",
            "    accuracy                           0.93     46435\n",
            "   macro avg       0.73      0.71      0.72     46435\n",
            "weighted avg       0.93      0.93      0.93     46435\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testc_truth, testc_pred = get_entity_true_pred(testc_sent, testc_pos, testc_chunk, testc_entity)\n",
        "\n",
        "print(accuracy(testc_truth, testc_pred))\n",
        "\n",
        "print(classification_report(testc_truth, testc_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELODSIdp3gZX",
        "outputId": "3b3d1182-cdc4-4cb8-f104-ee6766ed9fab"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy = 32 / 35 = 0.914286\n",
            "None\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.00      0.00      0.00         0\n",
            "        MISC       0.00      0.00      0.00         2\n",
            "           O       1.00      1.00      1.00        29\n",
            "         ORG       0.50      0.50      0.50         2\n",
            "         PER       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           0.91        35\n",
            "   macro avg       0.50      0.50      0.50        35\n",
            "weighted avg       0.91      0.91      0.91        35\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "empVq8qVNb98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 1\n",
        "\n",
        "#testa_sent[index], testa_pos[index], testa_chunk[index], testa_entity[index]\n",
        "\n",
        "y_hat = []\n",
        "sent, pos, chunk, entity = testa_sent[index], testa_pos[index], testa_chunk[index], testa_entity[index]\n",
        "\n",
        "for i, w in enumerate(sent):\n",
        "  pred = maxentClf.classify(get_conll_features(i, sent, pos, chunk))\n",
        "  print(\"{} --> {}\".format(entity[i], pred))\n",
        "  y_hat.append(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJYcIwQcNcKE",
        "outputId": "753e1821-45fc-4bab-f758-4022bd1d2575"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOC --> LOC\n",
            "O --> O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XJDbP9IK2u4w"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def entity_count (expected):\n",
        "\n",
        "    n_org = 0\n",
        "    n_per = 0\n",
        "    n_loc = 0\n",
        "    n_misc = 0\n",
        "    n_o = 0\n",
        "    \n",
        "    for e in expected:\n",
        "        if e == 'ORG':\n",
        "            n_org = n_org + 1\n",
        "        elif e == 'PER':\n",
        "            n_per = n_per + 1\n",
        "        elif e == 'LOC':\n",
        "            n_loc = n_loc + 1\n",
        "        elif e == 'MISC':\n",
        "            n_misc = n_misc + 1\n",
        "        elif e == 'O':\n",
        "            n_o = n_o + 1\n",
        "    \n",
        "    print('ORG:', n_org)\n",
        "    print('PER:', n_per)\n",
        "    print('LOC:', n_loc)\n",
        "    print('MISC:', n_misc)\n",
        "    print('O:', n_o)"
      ],
      "metadata": {
        "id": "tUP_HJCmnsld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entity_count(test_truth)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oej9XWr1tgLC",
        "outputId": "19402528-45be-4599-d639-925cedd22361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORG: 4590\n",
            "PER: 5924\n",
            "LOC: 4019\n",
            "MISC: 2188\n",
            "O: 81111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "U3mkObg0tj3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NEEL 2006 Dataset"
      ],
      "metadata": {
        "id": "DeGzs0DyoBlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_neel_features(index, sentence):\n",
        "  \"\"\"Function used to extract features for the NEEL dataset\n",
        "    \n",
        "    'w' represents word feature\n",
        "    '-n' represents previous 'n' feature\n",
        "    '+n' represents posterior 'n' feature\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  features = {}\n",
        "  last_index = len(sentence) - 1\n",
        "  word = sentence[index]\n",
        "  word_lc = word.lower()\n",
        "\n",
        "  # features from current word:\n",
        "\n",
        "  features[\"w\"] = word\n",
        "  features[\"length\"] = len(word)\n",
        "  features[\"upperase\"] = any(x.isupper() for x in word)\n",
        "  features[\"firstletter\"] = word[0].isupper() and (len(word)>1)\n",
        "  features[\"hasdigits\"] = any(x.isdigit() for x in word)\n",
        "  features[\"log_flag\"] = (\"field\" in word_lc) or (\"land\" in word_lc) or (\"burgh\" in word_lc) or (\"shire\" in word_lc)\n",
        "  features[\"hasdot\"] = (\".\" in word and len(word) > 1)\n",
        "  features[\"endsinns\"] = (len(word)>1 and word_lc[-2:]=='ns')\n",
        "\n",
        "\n",
        "  # features from previous 2 words\n",
        "  if index == 0: #first word in sentence\n",
        "    features[\"w-2\"] = \"<B>\"\n",
        "    features[\"w-1\"] = \"<B>\"\n",
        "\n",
        "  elif index == 1: #second word in sentence\n",
        "    features[\"w-2\"] = \"<B>\"\n",
        "    features[\"w-1\"] = sentence[0]\n",
        "\n",
        "  else:\n",
        "    features[\"w-2\"] = sentence[index-2]\n",
        "    features[\"w-1\"] = sentence[index-1]\n",
        "\n",
        "  # features from posterior 2 words\n",
        "  if index == last_index: #last word in sentence\n",
        "    features[\"w+2\"] = \"<E>\"\n",
        "    features[\"w+1\"] = \"<E>\"\n",
        "\n",
        "  elif index == last_index - 1: #second last word in sentence\n",
        "    features[\"w+2\"] = \"<E>\"\n",
        "    features[\"w+1\"] = sentence[last_index]\n",
        "  else:\n",
        "    features[\"w+2\"] = sentence[index+2]\n",
        "    features[\"w+1\"] = sentence[index+1]\n",
        "\n",
        "  return features\n"
      ],
      "metadata": {
        "id": "3w52jbCun2_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqJ34Svdp-QW",
        "outputId": "2b42f65b-c8ce-4f0d-ce27-f4e922802a3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def neel_sentences(gs_file, tsv_file):\n",
        "    \"\"\"NEEL2006 sentences from gs and tsv files\n",
        "    \n",
        "    Seperating NEEL data into individual sentences with corresponding tags\n",
        "    \n",
        "    arguments: gs_file, tsv_file\n",
        "    returns: sentences, entities, unknown tweet IDs\n",
        "    \"\"\"\n",
        "    \n",
        "    gs_col_names=['tweet_id','start','end','uri', 'confidence', 'entity']\n",
        "    tsv_col_names=['tweet_id','text']\n",
        "    tweets_dict = {}\n",
        "    data_dict = {}\n",
        "    seen_ids = set()\n",
        "    sent = []\n",
        "    entity = []\n",
        "    unknown_indicies = set()\n",
        "    \n",
        "    gs_df = pd.read_table(gs_file, sep = '\\t', header=None, names=gs_col_names)\n",
        "    # fixes entity label at index 4805 that is incorrect\n",
        "    if len(gs_df['entity']) > 4805 and gs_df['entity'][4805] == 'Organization373937812812615000':\n",
        "        gs_df.at[4805, 'entity'] = 'Organization'\n",
        "    \n",
        "    tsv_df = pd.read_table(tsv_file, sep = ',', header=None, names=tsv_col_names)\n",
        "    # strip '|' character from the edges of tsv_df column values\n",
        "    tsv_df['tweet_id'] = tsv_df['tweet_id'].apply(lambda x: str(x).strip('|'))\n",
        "    tsv_df['text'] = tsv_df['text'].apply(lambda x: str(x).strip('|'))\n",
        "\n",
        "    for index, row in tsv_df.iterrows():\n",
        "        tweets_dict[row['tweet_id']] = row['text']\n",
        "    \n",
        "    for index, row in gs_df.iterrows():\n",
        "        tweet_id = str(row['tweet_id'])\n",
        "        start = row['start']\n",
        "        end = row['end']\n",
        "        old_ent = row['entity']\n",
        "        \n",
        "        # Rename entity values as PER, LOC, ORG, MISC, O\n",
        "        if old_ent in ('Character', 'Person'):\n",
        "            new_ent = 'PER'\n",
        "        elif old_ent == 'Location':\n",
        "            new_ent = 'LOC'\n",
        "        elif old_ent == 'Organization':\n",
        "            new_ent = 'ORG'\n",
        "        else:\n",
        "            new_ent = 'MISC'\n",
        "        \n",
        "        try:\n",
        "            text = tweets_dict[tweet_id]\n",
        "            if tweet_id not in seen_ids:\n",
        "                seen_ids.add(tweet_id)\n",
        "                words = word_tokenize(text)\n",
        "                labels = ['O']*len(words)\n",
        "            else:\n",
        "                words = data_dict[tweet_id]['words']\n",
        "                labels = data_dict[tweet_id]['labels']\n",
        "            assert(len(words)==len(labels))\n",
        "            ent_words = word_tokenize(text[start:end])\n",
        "            for e in ent_words:\n",
        "                for i in range(len(words)):\n",
        "                    if e == words[i]:\n",
        "                        labels[i] = new_ent\n",
        "            data_dict[tweet_id] = {'words': words, 'labels': labels}\n",
        "        except KeyError:\n",
        "            unknown_indicies.add(tweet_id)\n",
        "    \n",
        "    for key in data_dict:\n",
        "        sent.append(data_dict[key]['words'])\n",
        "        entity.append(data_dict[key]['labels'])\n",
        "    \n",
        "    return sent, entity, unknown_indicies"
      ],
      "metadata": {
        "id": "uKcAgFk4o59p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gs_file = \"/content/drive/MyDrive/Dataset/NER_Dataset/NEEL2006/training_neel.gs\"\n",
        "train_tsv_file = \"/content/drive/MyDrive/Dataset/NER_Dataset/NEEL2006/training.tsv\"\n",
        "\n",
        "n_train_sent, n_train_ent, n_train_err = neel_sentences(train_gs_file, train_tsv_file)\n",
        "\n",
        "train_data = []\n",
        "for sent, entity in zip(n_train_sent, n_train_ent):\n",
        "  if len(sent) != len(entity):\n",
        "    raise ValueError(\"ERROR: NEEL train length miss match\")\n",
        "  for i, ent in enumerate(entity):\n",
        "    labelled_data = (get_neel_features(i, sent), ent)\n",
        "    train_data.append(labelled_data)"
      ],
      "metadata": {
        "id": "v4_OqTfSojhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZMcTszecp3eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import os\n",
        "from nltk import MaxentClassifier\n",
        "\n",
        "megam_path = os.path.expanduser(\"/content/megam_i686.opt\")\n",
        "nltk.config_megam(megam_path)\n",
        "maxentClf = MaxentClassifier.train(train_data, algorithm='MEGAM')"
      ],
      "metadata": {
        "id": "jIyaOmHzqb2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3BvoBsn5rhKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_gs_file = \"/content/drive/MyDrive/Dataset/NER_Dataset/NEEL2006/test_neel.gs\"\n",
        "test_tsv_file = \"/content/drive/MyDrive/Dataset/NER_Dataset/NEEL2006/test.tsv\"\n",
        "\n",
        "test_sent, test_ent, test_err = neel_sentences(test_gs_file, test_tsv_file)\n",
        "\n",
        "test_truth = []\n",
        "test_pred = []\n",
        "\n",
        "for sent, entity in zip(test_sent, test_ent):\n",
        "  if len(sent) != len(entity):\n",
        "    raise ValueError(\"ERROR: Neel test length miss match\")\n",
        "\n",
        "  for i, ent in enumerate(entity):\n",
        "    test_truth.append(ent)\n",
        "    pred = maxentClf.classify(get_neel_features(i, sent))\n",
        "    test_pred.append(pred)\n"
      ],
      "metadata": {
        "id": "jBGt1dm1q1k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy(test_truth, test_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_rVkiRAtB8c",
        "outputId": "3e94f37d-0590-4ab2-8c06-502fbff9f905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy = 4678 / 5410 = 0.864695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(test_truth, test_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti7EqH55tHwi",
        "outputId": "d37efc5e-683a-4167-ba8c-5932f1ede3b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.69      0.24      0.36        37\n",
            "        MISC       0.50      0.50      0.50       484\n",
            "           O       0.91      0.99      0.95      4363\n",
            "         ORG       0.36      0.03      0.05       146\n",
            "         PER       0.62      0.32      0.42       380\n",
            "\n",
            "    accuracy                           0.86      5410\n",
            "   macro avg       0.62      0.41      0.46      5410\n",
            "weighted avg       0.84      0.86      0.84      5410\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entity_count(test_truth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOmO75yStNZj",
        "outputId": "fe70aa6b-651f-405f-953a-e6dde2136359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORG: 146\n",
            "PER: 380\n",
            "LOC: 37\n",
            "MISC: 484\n",
            "O: 4363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nLDUcNhjtaOb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}