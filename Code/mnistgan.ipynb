{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random\n\n\ndef generate_batches(X_dataset, y_dataset, batch_size):\n    ind = 0\n    list_index = list(range(0, len(X_dataset)))\n    random.shuffle(list_index)\n    X_dataset = X_dataset[list_index]\n    y_dataset = y_dataset[list_index]\n    while ind < X_dataset.shape[0]:\n        if ind + batch_size > X_dataset.shape[0]:\n            X = X_dataset[ind:]\n            y = y_dataset[ind:]\n        else:\n            X = X_dataset[ind:ind + batch_size]\n            y = y_dataset[ind:ind + batch_size]\n        ind += batch_size\n        yield X, y","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-14T21:31:11.714393Z","iopub.execute_input":"2021-11-14T21:31:11.715117Z","iopub.status.idle":"2021-11-14T21:31:11.735578Z","shell.execute_reply.started":"2021-11-14T21:31:11.715030Z","shell.execute_reply":"2021-11-14T21:31:11.734899Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class Config:\n    IMG_H = 28\n    IMG_W = 28\n    IMG_C = 1\n    IMG_FLATTEN_SHAPE = IMG_H*IMG_W*IMG_C\n    BATCH_SIZE = 256\n    EPOCHS = 100\n    HIDDEN_LAYERS_NEURONS = [16, 16]\n    CNN_HIDDEN_LAYERS_NEURONS = [256,128,64]\n    LEARNING_RATE = 0.001\n    LATENT_DIM = 10\n    PLOTTING_SAVING_STEP = 10\n    NUMBER_TEST = 10\n\n\ndef recalculate_dim(H, W, C):\n    Config.IMG_H = H\n    Config.IMG_W = W\n    Config.IMG_C = C\n    Config.IMG_FLATTEN_SHAPE = H * W * C","metadata":{"execution":{"iopub.status.busy":"2021-11-14T21:31:13.068422Z","iopub.execute_input":"2021-11-14T21:31:13.068935Z","iopub.status.idle":"2021-11-14T21:31:13.075327Z","shell.execute_reply.started":"2021-11-14T21:31:13.068897Z","shell.execute_reply":"2021-11-14T21:31:13.074505Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\n\n\ndef create_class_res_directory(sub_directory):\n    if not os.path.exists(sub_directory):\n        os.makedirs(sub_directory)\n    if not os.path.exists(os.path.join(sub_directory,\"plots\")):\n        os.makedirs(os.path.join(sub_directory,\"plots\"))\n\n\ndef save_loss_file(_file, itr, dloss, gloss):\n    _file.write(\"%d,%f,%f\\n\"%(itr,dloss,gloss))\n\n\ndef create_loss_file(sub_directory):\n    file = open(os.path.join(sub_directory, 'loss_logs.csv'), 'w')\n    file.write('Iteration,Discriminator Loss,Generator Loss\\n')\n    return file","metadata":{"execution":{"iopub.status.busy":"2021-11-14T21:31:13.521078Z","iopub.execute_input":"2021-11-14T21:31:13.521718Z","iopub.status.idle":"2021-11-14T21:31:13.529996Z","shell.execute_reply.started":"2021-11-14T21:31:13.521682Z","shell.execute_reply":"2021-11-14T21:31:13.529318Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n\ndef sample_Z(m, n):\n    return np.random.uniform(-1, 1, size=[m, n])","metadata":{"execution":{"iopub.status.busy":"2021-11-14T21:31:13.893078Z","iopub.execute_input":"2021-11-14T21:31:13.893319Z","iopub.status.idle":"2021-11-14T21:31:13.897995Z","shell.execute_reply.started":"2021-11-14T21:31:13.893292Z","shell.execute_reply":"2021-11-14T21:31:13.897320Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import tensorflow.compat.v1 as tf\n\n\ndef _l2normalize(v, eps=1e-12):\n    return v / (tf.reduce_sum(v ** 2) ** 0.5 + eps)\n\n\ndef spectral_normed_weight(weights, num_iters=1, update_collection=None, with_sigma=False):\n    w_shape = weights.shape.as_list()\n    w_mat = tf.reshape(weights, [-1, w_shape[-1]])\n    u = tf.get_variable('u', [1, w_shape[-1]],\n                        initializer=tf.truncated_normal_initializer(),\n                        trainable=False)\n    u_ = u\n    v_ = tf.ones_like(w_mat)\n    for _ in range(num_iters):\n        v_ = _l2normalize(tf.matmul(u_, w_mat, transpose_b=True))\n        u_ = _l2normalize(tf.matmul(v_, w_mat))\n\n    sigma = tf.squeeze(tf.matmul(tf.matmul(v_, w_mat), u_, transpose_b=True))\n    w_mat /= sigma\n    if update_collection is None:\n        with tf.control_dependencies([u.assign(u_)]):\n            w_bar = tf.reshape(w_mat, w_shape)\n    else:\n        w_bar = tf.reshape(w_mat, w_shape)\n        if update_collection != 'NO_OPS':\n            tf.add_to_collection(update_collection, u.assign(u_))\n    if with_sigma:\n        return w_bar, sigma\n    else:\n        return w_bar\n\n\ndef snconv2d(x, filters, kernel_size=4, strides=2, padding='SAME', use_bias=False, sn=True, name=\"snconv2d\"):\n    with tf.variable_scope(name):\n        if sn:\n            w = tf.get_variable(\"kernel\", shape=[kernel_size, kernel_size, x.get_shape()[-1], filters],\n                                initializer=tf.random_normal_initializer(mean=0.0, stddev=0.02),\n                                regularizer=None)\n            bias = tf.get_variable(\"bias\", [filters], initializer=tf.constant_initializer(0.0))\n            x = tf.nn.conv2d(input=x, filter=spectral_normed_weight(w),\n                             strides=[1, strides, strides, 1], padding=padding)\n            if use_bias:\n                x = tf.nn.bias_add(x, bias)\n\n        else:\n            x = tf.layers.conv2d(inputs=x, filters=filters,\n                                 kernel_size=kernel_size,\n                                 strides=strides, use_bias=use_bias, padding=padding)\n    return x\n\n\ndef sndeconv2d(x, batch_size, filters, kernel_size=4, strides=2, padding='SAME', use_bias=False, sn=True, name=\"sndeconv2d\"):\n    with tf.variable_scope(name):\n        x_shape = x.get_shape().as_list()\n        output_shape = [batch_size, x_shape[1] * strides, x_shape[2] * strides, filters]\n        if sn:\n            w = tf.get_variable(\"kernel\", shape=[kernel_size, kernel_size, filters, x.get_shape()[-1]],\n                                initializer=tf.random_normal_initializer(mean=0.0, stddev=0.02),\n                                regularizer=None)\n            x = tf.nn.conv2d_transpose(x, filter=spectral_normed_weight(w), output_shape=output_shape,\n                                       strides=[1, strides, strides, 1], padding=padding)\n\n            if use_bias:\n                bias = tf.get_variable(\"bias\", [filters], initializer=tf.constant_initializer(0.0))\n                x = tf.nn.bias_add(x, bias)\n\n        else:\n            x = tf.layers.conv2d_transpose(inputs=x, filters=filters,\n                                           kernel_size=kernel_size,\n                                           strides=strides, padding=padding, use_bias=use_bias)\n        return x\n\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-14T21:31:14.259412Z","iopub.execute_input":"2021-11-14T21:31:14.259795Z","iopub.status.idle":"2021-11-14T21:31:18.391185Z","shell.execute_reply.started":"2021-11-14T21:31:14.259763Z","shell.execute_reply":"2021-11-14T21:31:18.390478Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import tensorflow_datasets as tfds\n\n\ndef get_mnist_dataset():\n    X_train, y_train = tfds.as_numpy(tfds.load('mnist', split='train', batch_size=-1, shuffle_files=True,\n                                               as_supervised=True))\n    X_train = scale_images(X_train)\n\n    recalculate_dim(X_train.shape[1], X_train.shape[2], X_train.shape[3])\n\n    print(X_train.shape, y_train.shape)\n    return X_train, y_train\n\n\ndef scale_images(X_dataset):\n    return (X_dataset - 127.5) / 127.5\n\n\ndef flatten_x_dataset(X_dataset):\n    return X_dataset.reshape(-1,Config.IMG_FLATTEN_SHAPE)\n\n\ndef reconstruct_x_dataset_img(X_dataset):\n    return X_dataset.reshape(-1, Config.IMG_H, Config.IMG_W, Config.IMG_C)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T21:31:18.393775Z","iopub.execute_input":"2021-11-14T21:31:18.394189Z","iopub.status.idle":"2021-11-14T21:31:19.473398Z","shell.execute_reply.started":"2021-11-14T21:31:18.394151Z","shell.execute_reply":"2021-11-14T21:31:19.472566Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import os\n\n\ndef run_models(X_train, y_train):\n    print(\"********************************************\\n\")\n    print(\"Mnist Gan Models\")\n    print(\"********************************************\\n\")\n    print(\"Enter \\n\"\n          \"1- for model q1 (GanLinearNoNormalization)\\n\"\n          \"2- for model q1 (GanLinearWithBatchNormalization)\\n\"\n          \"3- for model q1 (Gan2DCnn2DBatchNormalization)\\n\"\n          \"4- for model q1 (Gan2DCnnSpectralNormedWeight)\\n\"\n          \"else for model q1 (GanLinearNoNormalization)\\n\")\n    model_num = int(input())\n    model = None\n\n    if model_num == 2:\n        model = GanLinearWithBatchNormalization()\n        model.train(X_train, y_train, flatten_images=True)\n    elif model_num == 3:\n        model = Gan2DCnn2DBatchNormalization()\n        model.train(X_train, y_train, flatten_images=False)\n    elif model_num == 4:\n        model = Gan2DCnnSpectralNormedWeight()\n        model.train(X_train, y_train, flatten_images=False)\n    else:\n        model = GanLinearNoNormalization()\n        model.train(X_train, y_train, flatten_images=True)\n\n    Z_batch = sample_Z(Config.NUMBER_TEST, Config.LATENT_DIM)\n    G_yhat = model.predict(Z_batch)\n    visualize_reconstructed_img(os.path.join(model.sub_directory, \"plots\"), G_yhat, itr=-1, save_fig=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T21:31:19.474841Z","iopub.execute_input":"2021-11-14T21:31:19.475106Z","iopub.status.idle":"2021-11-14T21:31:19.487467Z","shell.execute_reply.started":"2021-11-14T21:31:19.475073Z","shell.execute_reply":"2021-11-14T21:31:19.484397Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport os\n\n\ndef visualize_reconstructed_img(sub_root, g_plot, itr=-1, save_fig=False):\n    num_row = 2\n    num_col = 5\n\n    g_plot = reconstruct_x_dataset_img(g_plot)\n\n    fig, ax = plt.subplots(num_row, num_col)\n    i = 0\n\n    for row in range(num_row):\n        for col in range(num_col):\n            plt.sca(ax[row, col])\n            plt.imshow(g_plot[i], cmap='gray')\n            plt.axis('off')\n            i += 1\n\n    plt.title('Samples at Iteration %d' % itr, loc='left')\n    if save_fig:\n        plt.savefig(os.path.join(sub_root,'iteration_%d.png' % itr))\n        plt.close()\n    else:\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-14T21:31:19.489883Z","iopub.execute_input":"2021-11-14T21:31:19.490345Z","iopub.status.idle":"2021-11-14T21:31:19.503843Z","shell.execute_reply.started":"2021-11-14T21:31:19.490307Z","shell.execute_reply":"2021-11-14T21:31:19.503073Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import os\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\ntf.set_random_seed(1)\n\n\nclass GanModel(object):\n    _sess = None\n\n    def _generator(self, Z, batch_size=tf.Variable(Config.BATCH_SIZE), hsize=Config.HIDDEN_LAYERS_NEURONS, reuse=False):\n        pass\n\n    def _discriminator(self, X, batch_size=tf.Variable(Config.BATCH_SIZE), hsize=Config.HIDDEN_LAYERS_NEURONS, reuse=False):\n        pass\n\n    def __init__(self):\n        tf.reset_default_graph()\n\n        self._G_sample = None\n        self._Batch_size = tf.Variable(0)\n        self._X = None\n        self._Z = None\n        self._disc_step = None\n        self._disc_loss = None\n        self._gen_step = None\n        self._gen_loss = None\n        self.sub_directory = \"\"\n\n    def predict(self, Z_batch):\n        G_yhat = self._sess.run(self._G_sample, feed_dict={self._Z: Z_batch,\n                                                           self._Batch_size: Z_batch.shape[0]})\n        return G_yhat\n\n    def train(self, X_train, y_train, flatten_images=True):\n        f = create_loss_file(self.sub_directory)\n        current_batch_size = Config.BATCH_SIZE\n        dloss = 0\n        gloss = 0\n        for i in range(Config.EPOCHS + 1):\n            for X_batch, y_batch in generate_batches(X_train, y_train, Config.BATCH_SIZE):\n                if flatten_images:\n                    X_batch = flatten_x_dataset(X_batch)\n                current_batch_size = X_batch.shape[0]\n                Z_batch = sample_Z(current_batch_size, Config.LATENT_DIM)\n\n                _, dloss = self._sess.run([self._disc_step, self._disc_loss], feed_dict={self._X: X_batch,\n                                                                                         self._Z: Z_batch,\n                                                                                         self._Batch_size: current_batch_size})\n                _, gloss = self._sess.run([self._gen_step, self._gen_loss], feed_dict={self._Z: Z_batch,\n                                                                                       self._Batch_size: current_batch_size})\n\n            save_fig = False\n\n            if i == int((Config.EPOCHS + 1) / 2) or i == Config.EPOCHS:\n                save_fig = True\n\n            if i % Config.PLOTTING_SAVING_STEP == 0:\n                print(\"Iterations: %d\\t Discriminator loss: %.4f\\t Generator loss: %.4f\" % (i, dloss, gloss))\n                save_loss_file(f, i, dloss, gloss)\n\n            if save_fig or i % Config.PLOTTING_SAVING_STEP == 0:\n                Z_batch = sample_Z(current_batch_size, Config.LATENT_DIM)\n                G_yhat = self.predict(Z_batch)\n                visualize_reconstructed_img(os.path.join(self.sub_directory, \"plots\"), G_yhat, itr=i, save_fig=save_fig)\n\n        f.close()","metadata":{"execution":{"iopub.status.busy":"2021-11-14T21:31:19.505550Z","iopub.execute_input":"2021-11-14T21:31:19.505861Z","iopub.status.idle":"2021-11-14T21:31:19.544641Z","shell.execute_reply.started":"2021-11-14T21:31:19.505824Z","shell.execute_reply":"2021-11-14T21:31:19.544009Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\n\nclass GanLinearNoNormalization(GanModel):\n\n    def _generator(self, Z, batch_size=tf.Variable(Config.BATCH_SIZE), hsize=Config.HIDDEN_LAYERS_NEURONS, reuse=False):\n        with tf.variable_scope(\"GAN/Generator\", reuse=reuse):\n            h1 = tf.layers.dense(Z, hsize[0])\n            h2 = tf.layers.dense(h1, hsize[1])\n            out = tf.layers.dense(h2, Config.IMG_FLATTEN_SHAPE)\n        return out\n\n    def _discriminator(self, X, batch_size=tf.Variable(Config.BATCH_SIZE), hsize=Config.HIDDEN_LAYERS_NEURONS, reuse=False):\n        with tf.variable_scope(\"GAN/Discriminator\", reuse=reuse):\n            h1 = tf.layers.dense(X, hsize[1])\n            h2 = tf.layers.dense(h1, hsize[0])\n            h3 = tf.layers.dense(h2, Config.IMG_FLATTEN_SHAPE)\n            out = tf.layers.dense(h3, 1)\n        return out, h3\n\n    def __init__(self):\n        super(GanLinearNoNormalization, self).__init__()\n\n        self.sub_directory = \"GanLinearNoNormalization\"\n        create_class_res_directory(self.sub_directory)\n\n        self._X = tf.placeholder(tf.float32, [None, Config.IMG_FLATTEN_SHAPE])\n        self._Z = tf.placeholder(tf.float32, [None, Config.LATENT_DIM])\n\n        self._G_sample = self._generator(self._Z)\n        r_logits, r_rep = self._discriminator(self._X)\n        f_logits, g_rep = self._discriminator(self._G_sample, reuse=True)\n\n        real_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=r_logits,\n                                                            labels=tf.ones_like(r_logits))\n        fake_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=f_logits,\n                                                            labels=tf.zeros_like(f_logits))\n        self._disc_loss = tf.reduce_mean(real_loss + fake_loss)\n        self._gen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=f_logits,\n                                                                                labels=tf.ones_like(f_logits)))\n\n        gen_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"GAN/Generator\")\n        disc_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"GAN/Discriminator\")\n\n        self._gen_step = tf.train.RMSPropOptimizer(learning_rate=Config.LEARNING_RATE).minimize(self._gen_loss,\n                                                                                                var_list=gen_vars)\n        self._disc_step = tf.train.RMSPropOptimizer(learning_rate=Config.LEARNING_RATE).minimize(self._disc_loss,\n                                                                                                 var_list=disc_vars)\n\n        self._sess = tf.Session()\n        tf.global_variables_initializer().run(session=self._sess)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-14T21:31:19.545632Z","iopub.execute_input":"2021-11-14T21:31:19.545854Z","iopub.status.idle":"2021-11-14T21:31:19.567507Z","shell.execute_reply.started":"2021-11-14T21:31:19.545822Z","shell.execute_reply":"2021-11-14T21:31:19.566852Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\n\nclass GanLinearWithBatchNormalization(GanModel):\n\n    def _generator(self, Z, batch_size=tf.Variable(Config.BATCH_SIZE), hsize=Config.HIDDEN_LAYERS_NEURONS, reuse=False):\n        with tf.variable_scope(\"GAN/Generator\", reuse=reuse):\n            h1 = tf.layers.dense(Z, hsize[0])\n            b1 = tf.layers.batch_normalization(h1)\n            h2 = tf.layers.dense(b1, hsize[1])\n            out = tf.layers.dense(h2, Config.IMG_FLATTEN_SHAPE)\n        return out\n\n    def _discriminator(self, X, batch_size=tf.Variable(Config.BATCH_SIZE), hsize=Config.HIDDEN_LAYERS_NEURONS, reuse=False):\n        with tf.variable_scope(\"GAN/Discriminator\", reuse=reuse):\n            h1 = tf.layers.dense(X, hsize[1])\n            b1 = tf.layers.batch_normalization(h1)\n            h2 = tf.layers.dense(b1, hsize[0])\n            h3 = tf.layers.dense(h2, Config.IMG_FLATTEN_SHAPE)\n            out = tf.layers.dense(h3, 1)\n        return out, h3\n\n    def __init__(self):\n        super(GanLinearWithBatchNormalization, self).__init__()\n\n        self.sub_directory = \"GanLinearWithBatchNormalization\"\n        create_class_res_directory(self.sub_directory)\n\n        self._X = tf.placeholder(tf.float32, [None, Config.IMG_FLATTEN_SHAPE])\n        self._Z = tf.placeholder(tf.float32, [None, Config.LATENT_DIM])\n\n        self._G_sample = self._generator(self._Z)\n        r_logits, r_rep = self._discriminator(self._X)\n        f_logits, g_rep = self._discriminator(self._G_sample, reuse=True)\n\n        real_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=r_logits,\n                                                            labels=tf.ones_like(r_logits))\n        fake_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=f_logits,\n                                                            labels=tf.zeros_like(f_logits))\n        self._disc_loss = tf.reduce_mean(real_loss + fake_loss)\n        self._gen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=f_logits,\n                                                                                labels=tf.ones_like(f_logits)))\n\n        gen_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"GAN/Generator\")\n        disc_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"GAN/Discriminator\")\n\n        self._gen_step = tf.train.RMSPropOptimizer(learning_rate=Config.LEARNING_RATE).minimize(self._gen_loss,\n                                                                                                var_list=gen_vars)\n        self._disc_step = tf.train.RMSPropOptimizer(learning_rate=Config.LEARNING_RATE).minimize(self._disc_loss,\n                                                                                                 var_list=disc_vars)\n\n        self._sess = tf.Session()\n        tf.global_variables_initializer().run(session=self._sess)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-14T21:31:19.568889Z","iopub.execute_input":"2021-11-14T21:31:19.569136Z","iopub.status.idle":"2021-11-14T21:31:19.590280Z","shell.execute_reply.started":"2021-11-14T21:31:19.569105Z","shell.execute_reply":"2021-11-14T21:31:19.589558Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\n\nclass Gan2DCnn2DBatchNormalization(GanModel):\n\n    def _generator(self, Z, batch_size=tf.Variable(Config.BATCH_SIZE), hsize=Config.CNN_HIDDEN_LAYERS_NEURONS, reuse=False):\n        with tf.variable_scope(\"GAN/Generator\", reuse=reuse):\n            x = tf.layers.dense(Z, 7 * 7 * hsize[0], use_bias=False)\n            x = tf.layers.batch_normalization(x)\n            x = tf.nn.leaky_relu(x)\n\n            x = tf.reshape(x, (tf.shape(Z)[0], 7, 7, hsize[0]))\n            # print(x.shape)\n\n            x = tf.layers.conv2d_transpose(x, hsize[1], kernel_size=5, strides=1, padding='same', use_bias=False)\n            x = tf.layers.batch_normalization(x)\n            x = tf.nn.leaky_relu(x)\n            # print(x.shape)\n\n            x = tf.layers.conv2d_transpose(x, hsize[2], kernel_size=5, strides=2, padding='same', use_bias=False)\n            x = tf.layers.batch_normalization(x)\n            x = tf.nn.leaky_relu(x)\n            # print(x.shape)\n\n            x = tf.layers.conv2d_transpose(x, 1, kernel_size=5, strides=2, padding='same', use_bias=False)\n            outputs = tf.nn.tanh(x)\n            # print(outputs.shape)\n\n        return outputs\n\n    def _discriminator(self, inputs, batch_size=tf.Variable(Config.BATCH_SIZE), hsize=Config.CNN_HIDDEN_LAYERS_NEURONS, reuse=False):\n        with tf.variable_scope(\"GAN/Discriminator\", reuse=reuse):\n            x = tf.layers.conv2d(inputs, hsize[0], kernel_size=5, strides=2, padding='same')\n            x = tf.nn.leaky_relu(x)\n            x = tf.layers.dropout(x, 0.3)\n            # print(x.shape)\n\n            x = tf.layers.conv2d(x, hsize[1], kernel_size=5, strides=2, padding='same')\n            x = tf.nn.leaky_relu(x)\n            x = tf.layers.dropout(x, 0.3)\n            # print(x.shape)\n\n            x = tf.layers.flatten(x)\n            outputs = tf.layers.dense(x, 1)\n        return outputs, x\n\n    def __init__(self):\n        super(Gan2DCnn2DBatchNormalization, self).__init__()\n\n        self.sub_directory = \"Gan2DCnn2DBatchNormalization\"\n        create_class_res_directory(self.sub_directory)\n\n        self._X = tf.placeholder(tf.float32, [None, Config.IMG_H, Config.IMG_W, Config.IMG_C])\n        self._Z = tf.placeholder(tf.float32, [None, Config.LATENT_DIM])\n\n        self._G_sample = self._generator(self._Z)\n        r_logits, r_rep = self._discriminator(self._X)\n        f_logits, g_rep = self._discriminator(self._G_sample, reuse=True)\n\n        real_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=r_logits,\n                                                            labels=tf.ones_like(r_logits))\n        fake_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=f_logits,\n                                                            labels=tf.zeros_like(f_logits))\n        self._disc_loss = tf.reduce_mean(real_loss + fake_loss)\n        self._gen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=f_logits,\n                                                                                labels=tf.ones_like(f_logits)))\n\n        gen_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"GAN/Generator\")\n        disc_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"GAN/Discriminator\")\n\n        self._gen_step = tf.train.RMSPropOptimizer(learning_rate=Config.LEARNING_RATE).minimize(self._gen_loss,\n                                                                                                var_list=gen_vars)\n        self._disc_step = tf.train.RMSPropOptimizer(learning_rate=Config.LEARNING_RATE).minimize(self._disc_loss,\n                                                                                                 var_list=disc_vars)\n\n        self._sess = tf.Session()\n        tf.global_variables_initializer().run(session=self._sess)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T21:31:19.593550Z","iopub.execute_input":"2021-11-14T21:31:19.593754Z","iopub.status.idle":"2021-11-14T21:31:19.620201Z","shell.execute_reply.started":"2021-11-14T21:31:19.593730Z","shell.execute_reply":"2021-11-14T21:31:19.619401Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\n\nclass Gan2DCnnSpectralNormedWeight(GanModel):\n\n    def _generator(self, Z, batch_size=tf.Variable(Config.BATCH_SIZE), hsize=Config.CNN_HIDDEN_LAYERS_NEURONS, reuse=False):\n        with tf.variable_scope(\"GAN/Generator\", reuse=reuse):\n            x = tf.layers.dense(Z, 7 * 7 * hsize[0], use_bias=False)\n            # x = tf.layers.batch_normalization(x)\n            x = tf.nn.leaky_relu(x)\n\n            x = tf.reshape(x, (tf.shape(Z)[0], 7, 7, hsize[0]))\n            # print(x.shape)\n\n            x = sndeconv2d(x, self._Batch_size, hsize[1], kernel_size=5, strides=1, padding='SAME', use_bias=False, sn=True,\n                           name=\"sndeconv2d_1\")\n            # x = tf.layers.batch_normalization(x)\n            x = tf.nn.leaky_relu(x)\n            # print(x.shape)\n\n            x = sndeconv2d(x, self._Batch_size, hsize[2], kernel_size=5, strides=2, padding='SAME', use_bias=False, sn=True,\n                           name=\"sndeconv2d_2\")\n            # x = tf.layers.batch_normalization(x)\n            x = tf.nn.leaky_relu(x)\n            # print(x.shape)\n\n            x = sndeconv2d(x, self._Batch_size, 1, kernel_size=5, strides=2, padding='SAME', use_bias=False, sn=True,\n                           name=\"sndeconv2d_3\")\n            outputs = tf.nn.tanh(x)\n        return outputs\n\n    def _discriminator(self, inputs, batch_size=tf.Variable(Config.BATCH_SIZE), hsize=Config.CNN_HIDDEN_LAYERS_NEURONS, reuse=False):\n        with tf.variable_scope(\"GAN/Discriminator\", reuse=reuse):\n            x = snconv2d(inputs, hsize[0], kernel_size=5, strides=2, padding='SAME', use_bias=False, sn=True,\n                         name=\"snconv2d_1\")\n            x = tf.nn.leaky_relu(x)\n            x = tf.layers.dropout(x, 0.3)\n            # print(x.shape)\n\n            x = snconv2d(x, hsize[1], kernel_size=5, strides=2, padding='SAME', use_bias=False, sn=True,\n                         name=\"snconv2d_2\")\n            x = tf.nn.leaky_relu(x)\n            x = tf.layers.dropout(x, 0.3)\n\n            x = tf.layers.flatten(x)\n            outputs = tf.layers.dense(x, 1)\n        return outputs, x\n\n    def __init__(self):\n        super(Gan2DCnnSpectralNormedWeight, self).__init__()\n\n        self.sub_directory = \"Gan2DCnnSpectralNormedWeight\"\n        create_class_res_directory(self.sub_directory)\n\n        self._X = tf.placeholder(tf.float32, [None, Config.IMG_H, Config.IMG_W, Config.IMG_C])\n        self._Z = tf.placeholder(tf.float32, [None, Config.LATENT_DIM])\n        self._Batch_size = tf.Variable(Config.BATCH_SIZE)\n\n        self._G_sample = self._generator(self._Z, self._Batch_size)\n        r_logits, r_rep = self._discriminator(self._X, self._Batch_size)\n        f_logits, g_rep = self._discriminator(self._G_sample, self._Batch_size, reuse=True)\n\n        real_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=r_logits,\n                                                            labels=tf.ones_like(r_logits))\n        fake_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=f_logits,\n                                                            labels=tf.zeros_like(f_logits))\n        self._disc_loss = tf.reduce_mean(real_loss + fake_loss)\n        self._gen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=f_logits,\n                                                                                labels=tf.ones_like(f_logits)))\n\n        gen_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"GAN/Generator\")\n        disc_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"GAN/Discriminator\")\n\n        self._gen_step = tf.train.RMSPropOptimizer(learning_rate=Config.LEARNING_RATE).minimize(self._gen_loss,\n                                                                                                var_list=gen_vars)\n        self._disc_step = tf.train.RMSPropOptimizer(learning_rate=Config.LEARNING_RATE).minimize(self._disc_loss,\n                                                                                                 var_list=disc_vars)\n\n        self._sess = tf.Session()\n        tf.global_variables_initializer().run(session=self._sess)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T21:31:19.622084Z","iopub.execute_input":"2021-11-14T21:31:19.622410Z","iopub.status.idle":"2021-11-14T21:31:19.651038Z","shell.execute_reply.started":"2021-11-14T21:31:19.622375Z","shell.execute_reply":"2021-11-14T21:31:19.650129Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    X_train, y_train = get_mnist_dataset()\n    run_models(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-14T21:31:19.653680Z","iopub.execute_input":"2021-11-14T21:31:19.654058Z","iopub.status.idle":"2021-11-14T21:55:45.741210Z","shell.execute_reply.started":"2021-11-14T21:31:19.654022Z","shell.execute_reply":"2021-11-14T21:55:45.740491Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}